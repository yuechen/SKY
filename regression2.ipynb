{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "#from gtrends import gtrends\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import yahoo_stock"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Constructing the raw dataset from which we want to work\n",
      "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
      "years = [str(y) for y in range(2004, 2014)]\n",
      "companies = [('AAPL', 'apple'), ('FB', 'facebook'), ('YHOO', 'yahoo')]\n",
      "\n",
      "ticker = 'AAPL'\n",
      "name = 'apple'\n",
      "\n",
      "stock_info = yahoo_stock.get_historical_prices(ticker, '20040101', '20131201')\n",
      "df = pd.DataFrame(stock_info[1:])\n",
      "df.columns = stock_info[0]\n",
      "df = df.set_index('Date')\n",
      "\n",
      "all_time_data = pd.DataFrame()\n",
      "for year in years:\n",
      "    for month in months:\n",
      "        date = year + '-' + month\n",
      "        filename = \"data/\" + name + \"_\" + date + \".csv\"\n",
      "        #gtrends.getGoogleTrendData(name, filename, date)\n",
      "        month_data = pd.DataFrame.from_csv(filename)\n",
      "        month_data.columns = ['Trend']\n",
      "        \n",
      "        all_time_data = pd.concat([all_time_data, month_data])\n",
      "\n",
      "df = df.join(all_time_data, how=\"outer\")\n",
      "\n",
      "df['Ticker'] = pd.Series([ticker] * len(df), index=df.index)\n",
      "df = df.reset_index()\n",
      "new_columns = list(df.columns)\n",
      "new_columns[0] = 'Date'\n",
      "df.columns = new_columns\n",
      "df.Date = df.Date.apply(lambda d: d.strftime('%Y-%m-%d'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "apple_complete_data = df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "\n",
      "last_weeks_prices = []\n",
      "last_weeks_trends = []\n",
      "apple_formatted_data = []\n",
      "current_month = '01'\n",
      "for key, row in (apple_complete_data[4:]).iterrows():\n",
      "    if key % 7 == 4:\n",
      "        stock_price = float(row['Open'])\n",
      "        current_month = row['Date'][5:7]\n",
      "        if len(last_weeks_prices) != 5 or len(last_weeks_trends) != 7 or math.isnan(stock_price):\n",
      "            last_weeks_prices = []\n",
      "            last_weeks_trends = []\n",
      "        else:\n",
      "            last_weeks_data = []\n",
      "            \n",
      "            normalizing_price = last_weeks_prices[4]\n",
      "            monday_price = (stock_price / normalizing_price) * 100\n",
      "            last_weeks_prices = [(p / normalizing_price) * 100 for p in last_weeks_prices]\n",
      "            \n",
      "            max_trend = max(last_weeks_trends)\n",
      "            last_weeks_trends = [(t / float(max_trend)) * 100 for t in last_weeks_trends]\n",
      "            \n",
      "            apple_formatted_data.append([monday_price] + last_weeks_prices + last_weeks_trends)\n",
      "            last_weeks_prices = []\n",
      "            last_weeks_trends = []\n",
      "    \n",
      "    stock_price = float(row['Open'])\n",
      "    if not math.isnan(stock_price):\n",
      "        last_weeks_prices.append(stock_price)\n",
      "    if current_month == row['Date'][5:7]:\n",
      "        last_weeks_trends.append(row['Trend'])\n",
      "\n",
      "print len(apple_formatted_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "311\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "outcomes = [r[0] for r in apple_formatted_data]\n",
      "predictors = [r[1:] for r in apple_formatted_data]\n",
      "\n",
      "from sklearn.cross_validation import train_test_split\n",
      "Xtrain, Xtest, Ytrain, Ytest = train_test_split(predictors, outcomes, test_size=0.1)\n",
      "\n",
      "from sklearn import linear_model\n",
      "import numpy as np\n",
      "def get_coefficients(predictors, outcomes):\n",
      "    \"\"\"\n",
      "    returns a tuple of intercept and coefficients\n",
      "    \"\"\"\n",
      "    regr = linear_model.LinearRegression()\n",
      "    regr.intercept = True \n",
      "    regr.fit(predictors, outcomes)\n",
      "    \n",
      "    return (regr.intercept_, regr.coef_)\n",
      "\n",
      "coefficients = get_coefficients(Xtrain, Ytrain)\n",
      "\n",
      "print coefficients\n",
      "\n",
      "def predict_tomorrow(coefs, today_data):\n",
      "    \"\"\" \n",
      "    coefs is a tuple of the intercept and coefficients of the model fitted\n",
      "    today_data is the gtrend/stock data over the same length of time in training\n",
      "    \"\"\"\n",
      "    intercept, coefficients = coefs \n",
      "    return (np.dot(coefficients, today_data) + intercept)\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def visualize_fit(coefs, predictors, outcomes):\n",
      "    predicted_prices = []\n",
      "    for row in predictors:\n",
      "        predicted_prices.append(predict_tomorrow(coefs, row))\n",
      "    plt.scatter(predicted_prices, outcomes)\n",
      "    correct = 0\n",
      "    total = 0\n",
      "    for p, o in zip(predicted_prices, outcomes):\n",
      "        p = p - 100\n",
      "        o = o - 100\n",
      "        if (p > 0 and o > 0 or p < 0 and o < 0):\n",
      "            correct += 1\n",
      "        total += 1\n",
      "    print \"percent correct:\", (float(correct) / float(total))\n",
      "    plt.plot([90,110],[90,110])\n",
      "    plt.show()\n",
      "    return\n",
      "\n",
      "visualize_fit(coefficients, Xtest, Ytest)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(84.286434546035338, array([ -1.68757706e-01,   2.40118862e-01,  -1.97992219e-01,\n",
        "         3.50088612e-01,  -8.55584868e-17,  -1.65261808e-02,\n",
        "        -3.84169483e-02,  -5.70719408e-02,  -4.71590254e-03,\n",
        "         4.50436304e-02,  -1.13256995e-01,   1.13290472e-01]))\n",
        "percent correct:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.59375\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD9CAYAAABdoNd6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUgE2eyIiFWCzJyJSgf8HDGfzJ0Eou/sAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10886f7d0>"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "![Alt text]['files/images/fig1.png']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/bin/sh: [Alt: command not found\r\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predictors\n",
      "outcomes\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "[100.08609556607834,\n",
        " 100.75723830734968,\n",
        " 104.96515679442508,\n",
        " 106.69338677354709,\n",
        " 98.9385065885798,\n",
        " 99.25665101721441,\n",
        " 101.37037037037038,\n",
        " 96.46655231560892,\n",
        " 99.5667870036101,\n",
        " 98.94538606403013,\n",
        " 97.98165137614679,\n",
        " 101.44981412639405,\n",
        " 101.40845070422534,\n",
        " 103.35651648019353,\n",
        " 97.59146341463415,\n",
        " 97.84332381858547,\n",
        " 96.60194174757282,\n",
        " 101.30718954248366,\n",
        " 100.48844024747639,\n",
        " 98.0392156862745,\n",
        " 100.90287277701779,\n",
        " 98.6648865153538,\n",
        " 98.07886754297269,\n",
        " 99.59893048128342,\n",
        " 99.28481278923013,\n",
        " 101.27314814814814,\n",
        " 98.92453518045936,\n",
        " 100.34539174695512,\n",
        " 111.37141827356281,\n",
        " 100.90727356604646,\n",
        " 97.95032914422501,\n",
        " 107.43076923076922,\n",
        " 102.69898099697052,\n",
        " 103.59378913097923,\n",
        " 100.77095249937827,\n",
        " 99.90768520655435,\n",
        " 101.02974828375284,\n",
        " 95.57618787547788,\n",
        " 99.0499457111835,\n",
        " 101.05719707237735,\n",
        " 101.05263157894737,\n",
        " 101.61073825503355,\n",
        " 95.96256684491979,\n",
        " 98.38835456199637,\n",
        " 94.24405218726018,\n",
        " 101.07395655357578,\n",
        " 101.26611418047884,\n",
        " 97.72727272727272,\n",
        " 101.20028241939278,\n",
        " 106.94891854578923,\n",
        " 99.7191011235955,\n",
        " 98.15698178664356,\n",
        " 101.6324905434999,\n",
        " 103.70441458733207,\n",
        " 100.07733952049497,\n",
        " 99.90745881917452,\n",
        " 97.20267417311752,\n",
        " 98.50107066381157,\n",
        " 100.0,\n",
        " 99.24973204715968,\n",
        " 100.88936800970221,\n",
        " 98.5722206820072,\n",
        " 97.55997258396162,\n",
        " 102.22460877569806,\n",
        " 101.56128024980484,\n",
        " 100.72586872586872,\n",
        " 100.16597510373444,\n",
        " 99.09770195967855,\n",
        " 98.03490247836926,\n",
        " 102.00345921014701,\n",
        " 101.57250208739215,\n",
        " 99.29255711127489,\n",
        " 100.96427442301612,\n",
        " 97.09055246812683,\n",
        " 98.08344640434193,\n",
        " 99.07903549899531,\n",
        " 98.53333333333333,\n",
        " 102.40722166499498,\n",
        " 104.51986237097279,\n",
        " 101.29685275976594,\n",
        " 99.39447644365677,\n",
        " 101.72260172260172,\n",
        " 98.92761394101876,\n",
        " 99.34051144010768,\n",
        " 99.16689062080086,\n",
        " 99.4182202829565,\n",
        " 101.29162973280992,\n",
        " 97.84709480122324,\n",
        " 99.60502692998205,\n",
        " 100.30537937514683,\n",
        " 101.91447896365928,\n",
        " 98.43855313412716,\n",
        " 99.07014120078061,\n",
        " 98.31159757801585,\n",
        " 99.17792792792793,\n",
        " 100.78177350904622,\n",
        " 100.6855918585967,\n",
        " 99.63696369636962,\n",
        " 100.77016173396413,\n",
        " 101.94540639641474,\n",
        " 101.74494152589568,\n",
        " 100.07257552390456,\n",
        " 100.14306151645206,\n",
        " 102.20527275741999,\n",
        " 100.27452563584983,\n",
        " 102.48833592534992,\n",
        " 101.17190257677373,\n",
        " 98.7276831520624,\n",
        " 104.22352176738141,\n",
        " 101.59822965330709,\n",
        " 102.1910671876197,\n",
        " 101.77198506260527,\n",
        " 103.96060649000994,\n",
        " 100.77554744525547,\n",
        " 103.23293553071919,\n",
        " 103.04889270596895,\n",
        " 97.76744719926538,\n",
        " 100.08635112526309,\n",
        " 96.57026000584283,\n",
        " 100.48396854204475,\n",
        " 101.6007137608901,\n",
        " 100.18385249776752,\n",
        " 102.58257942352198,\n",
        " 100.86363636363637,\n",
        " 104.85747051114022,\n",
        " 101.30387841541402,\n",
        " 94.35632891900215,\n",
        " 101.0366713681241,\n",
        " 96.10398114195915,\n",
        " 101.9419306184012,\n",
        " 99.44346807264208,\n",
        " 101.11924000873553,\n",
        " 98.816474672558,\n",
        " 98.29255319148936,\n",
        " 99.80191097646238,\n",
        " 97.4296069138556,\n",
        " 102.21007747282447,\n",
        " 102.14851541574059,\n",
        " 99.03868976976027,\n",
        " 101.20947630922694,\n",
        " 103.78982057854265,\n",
        " 98.06188561215372,\n",
        " 100.187691957684,\n",
        " 94.11569809820422,\n",
        " 98.13464235624124,\n",
        " 95.76495076455048,\n",
        " 121.99533255542589,\n",
        " 100.18072289156628,\n",
        " 105.24742610428429,\n",
        " 100.93712212817414,\n",
        " 94.36860068259385,\n",
        " 104.00341755156839,\n",
        " 107.67017155506365,\n",
        " 103.4375,\n",
        " 100.08894818768069,\n",
        " 97.04967278189035,\n",
        " 103.07153164296021,\n",
        " 95.29092143989133,\n",
        " 100.23883696780894,\n",
        " 100.60730727789205,\n",
        " 96.56287535803382,\n",
        " 100.45387027562305,\n",
        " 98.60397946084724,\n",
        " 98.70582765034099,\n",
        " 101.15271419228256,\n",
        " 98.97460601472713,\n",
        " 97.98285426122037,\n",
        " 101.88310277395523,\n",
        " 102.62536662136061,\n",
        " 102.34707349273873,\n",
        " 102.81057150523209,\n",
        " 102.0516087926091,\n",
        " 100.10272524019577,\n",
        " 97.3859711801834,\n",
        " 101.47330748583359,\n",
        " 97.61421025134962,\n",
        " 99.1712855835979,\n",
        " 101.0219218724246,\n",
        " 101.0848282796211,\n",
        " 99.2078162133615,\n",
        " 99.01312591152164,\n",
        " 102.3011791595242,\n",
        " 101.28653817715778,\n",
        " 101.93321616871704,\n",
        " 98.78147436545657,\n",
        " 101.49091473831342,\n",
        " 101.1887779362815,\n",
        " 95.66839069027252,\n",
        " 101.58853761096402,\n",
        " 101.6651843067497,\n",
        " 102.35879780403833,\n",
        " 99.1247745964727,\n",
        " 98.07820632590418,\n",
        " 101.76894518453811,\n",
        " 100.31893302406493,\n",
        " 99.38045620951846,\n",
        " 101.45154669950371,\n",
        " 102.68351729514586,\n",
        " 99.81972095939801,\n",
        " 106.21080663173268,\n",
        " 103.1140474559884,\n",
        " 101.99816345270891,\n",
        " 98.84099829667483,\n",
        " 98.69657950864998,\n",
        " 101.13189933486328,\n",
        " 100.65439987681886,\n",
        " 98.38267434929466,\n",
        " 100.96234812943582,\n",
        " 99.59048603929679,\n",
        " 99.42021678850517,\n",
        " 100.64361520027387,\n",
        " 101.03870282129512,\n",
        " 103.58769190736406,\n",
        " 100.00647102598117,\n",
        " 99.33931564934426,\n",
        " 99.75156451460737,\n",
        " 97.6139240506329,\n",
        " 99.58112803195117,\n",
        " 101.47661504770844,\n",
        " 99.99067251189256,\n",
        " 101.44914518398753,\n",
        " 97.56806229479618,\n",
        " 100.57505285412263,\n",
        " 102.27318796513481,\n",
        " 99.66185151128644,\n",
        " 101.45947654207485,\n",
        " 98.27606495646035,\n",
        " 97.83978397839785,\n",
        " 99.47667934456234,\n",
        " 98.1311114968466,\n",
        " 97.17575686182118,\n",
        " 98.80502193314173,\n",
        " 96.46493814401653,\n",
        " 98.85928116606813,\n",
        " 101.17949995846831,\n",
        " 100.52276473011949,\n",
        " 102.61582911980187,\n",
        " 95.07149616233835,\n",
        " 100.4126220012167,\n",
        " 100.64610542010657,\n",
        " 104.58280572244524,\n",
        " 100.36911563937907,\n",
        " 99.895073448586,\n",
        " 100.8808345308425,\n",
        " 101.17793824820671,\n",
        " 99.51770911831198,\n",
        " 99.8560794044665,\n",
        " 99.20074493675797,\n",
        " 97.75150427530876,\n",
        " 99.70217640320733,\n",
        " 100.55473761699443,\n",
        " 100.3083224557771,\n",
        " 101.74555971973275,\n",
        " 100.8764998805608,\n",
        " 102.33445067724722,\n",
        " 99.88342853336441,\n",
        " 97.74879428305907,\n",
        " 96.48787581588827,\n",
        " 98.79848612557223,\n",
        " 99.5699115044248,\n",
        " 100.10113117087423,\n",
        " 102.82015395381386,\n",
        " 99.99299474605955,\n",
        " 99.69950262503454,\n",
        " 100.3598971722365,\n",
        " 96.96099701482798,\n",
        " 102.76690840159301,\n",
        " 100.75641253575989,\n",
        " 101.5640625,\n",
        " 103.10533577959396,\n",
        " 101.36094846078035,\n",
        " 97.78619324895716,\n",
        " 98.88176795580111,\n",
        " 97.24594107035477,\n",
        " 100.44316665607727,\n",
        " 97.0477775136677,\n",
        " 102.54061655749231,\n",
        " 102.95316070068546,\n",
        " 94.86808818214674,\n",
        " 98.86935405536668,\n",
        " 101.53765098444786,\n",
        " 96.48368522072937,\n",
        " 100.52742616033757,\n",
        " 99.98836668217776,\n",
        " 100.80378142625533,\n",
        " 102.22403097364601,\n",
        " 100.0824499411072,\n",
        " 98.35310376597951,\n",
        " 101.20370131711213,\n",
        " 102.59632512627803,\n",
        " 98.58942725506037,\n",
        " 98.37376153057738,\n",
        " 101.88545246277205,\n",
        " 99.09049150206707,\n",
        " 97.34999641568496,\n",
        " 102.89503270645952,\n",
        " 99.38267274640478,\n",
        " 99.15954744862617,\n",
        " 101.26349643923731,\n",
        " 99.61189604046747,\n",
        " 100.83774867539739,\n",
        " 99.49927474317961,\n",
        " 98.22303660459369,\n",
        " 103.7866108786611,\n",
        " 98.65021290669314,\n",
        " 100.58317419248854,\n",
        " 101.14231506551512,\n",
        " 99.57088007227281,\n",
        " 101.05134284270667,\n",
        " 99.69805157810778,\n",
        " 100.28872805666768]"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "OK.  Our data just doesn't look that great, so let's run an Extra Sum of Squares F-Test to determine whether our model is worth a darn.  We couldn't find a function to do this online, so we wrote one"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model\n",
      "import numpy as np\n",
      "\n",
      "red_pred = 10*[np.mean(y)]\n",
      "full_pred = []\n",
      "for i in x:\n",
      "    full_pred.append(i[0] * regr.coef_[0] + regr.intercept_)\n",
      "\n",
      "import scipy.stats\n",
      "def run_ESS_F_Test(f_mod_pred, red_mod_pred, actual, dfFull, dfRed):\n",
      "    SSRFull = 0.\n",
      "    SSRRed = 0.\n",
      "    for f_pred, act in zip(f_mod_pred, actual):\n",
      "        SSRFull += (f_pred - act)**2\n",
      "    for red_pred, act in zip(red_mod_pred, actual):\n",
      "        SSRRed += (red_pred - act)**2\n",
      "    sigsq = SSRFull/float(dfFull)\n",
      "    \n",
      "    ESS = SSRRed - SSRFull\n",
      "    FStat = ESS/float(dfRed-dfFull)/sigsq\n",
      "    print \"F-stat: \", FStat\n",
      "    print \"The probability of observing such an extreme result is: \", 1. - scipy.stats.f.cdf(FStat,dfRed-dfFull,dfFull)\n",
      "run_ESS_F_Test(full_pred,red_pred,y,8,9)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "F-stat:  4599.86857769\n",
        "The probability of observing such an extreme result is:  2.48612241904e-12\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Convinced that our test is right, let's run some F-tests.  First, let's compare our full model with the model that is lacking the google trends data.  this will tell us whether or not the google trends data is telling us anything"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "actuals = outcomes\n",
      "\n",
      "DFReduced = len(actuals) - 1\n",
      "DFFull = len(actuals) - 13\n",
      "DFMid = len(actuals) - 6\n",
      "\n",
      "red_pred = len(actuals)*[np.mean(actuals)]\n",
      "predictor_lists = predictors\n",
      "\n",
      "mid_predictors = []\n",
      "for row in predictor_lists:\n",
      "    mid_predictors.append(row[0:5])\n",
      "from sklearn import linear_model\n",
      "\n",
      "#Get the mid level predictions\n",
      "mid_regr = linear_model.LinearRegression()\n",
      "mid_regr.fit(mid_predictors, actuals)\n",
      "mid_coeffs = mid_regr.coef_\n",
      "mid_intercept = mid_regr.intercept_\n",
      "mid_predictions =[]\n",
      "for datum in mid_predictors:\n",
      "    mid_predictions.append(mid_intercept + np.dot(datum, mid_coeffs))\n",
      "\n",
      "#Get the full level predictions\n",
      "full_regr = linear_model.LinearRegression()\n",
      "full_regr.fit(predictor_lists, actuals)\n",
      "full_coeffs = full_regr.coef_\n",
      "full_intercept = full_regr.intercept_\n",
      "full_predictions = []\n",
      "for datum in predictor_lists:\n",
      "    full_predictions.append(full_intercept + np.dot(datum, full_coeffs))\n",
      "print DFFull\n",
      "print DFMid\n",
      "print DFReduced"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "298\n",
        "305\n",
        "310\n"
       ]
      }
     ],
     "prompt_number": 74
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, it kind of looks like our model isn't actually any good.  So, let's run an ESS F-Test comparing the full model with the equal mean model (basically, just always guessing that the average percentage of Friday's stock will always be the same)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "run_ESS_F_Test(full_predictions, red_pred, actuals, DFFull, DFReduced)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "F-stat:  4.47785555366\n",
        "The probability of observing such an extreme result is:  1.27942434136e-06\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looks like our model actually is good for something!\n",
      "\n",
      "but it sure looks like Google Trends isn't really telling us anything, so let's test the full model (stock prices from the past week and also Google Trends data) against the reduced model of just stock prices. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "run_ESS_F_Test(full_predictions, mid_predictions, actuals, DFFull, DFMid)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "F-stat:  3.87032665642\n",
        "The probability of observing such an extreme result is:  0.000474275972273\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Well, this is surprising.  This seems to imply that the Google Trends actually DO make a difference.  But, frankly, I'm unconvinced.  I wonder whether this is just because we added extra parameters to our linear model.  So, I'm going to run a similar test, but just make up the google trends data in a completely random fashion.  If we get the same thing, then we know that this F-Test is messed up (Maybe some assumption somewhere is broken).\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "mid_predictors = []\n",
      "for row in predictor_lists:\n",
      "    mid_predictors.append(row[0:5])\n",
      "made_up_G_trend_predictors = []\n",
      "for predictor_row in mid_predictors:\n",
      "    dummy_list = []\n",
      "    for element in predictor_row:\n",
      "        dummy_list.append(element)\n",
      "    for i in xrange(7):\n",
      "        dummy_list.append(random.randrange(100))\n",
      "    made_up_G_trend_predictors.append(dummy_list)\n",
      "\n",
      "fake_regr = linear_model.LinearRegression()\n",
      "fake_regr.fit(made_up_G_trend_predictors, actuals)\n",
      "fake_coeffs = fake_regr.coef_\n",
      "fake_intercept = fake_regr.intercept_\n",
      "fake_predictions =[]\n",
      "for datum in made_up_G_trend_predictors:\n",
      "    fake_predictions.append(fake_intercept + np.dot(datum, fake_coeffs))\n",
      "\n",
      "run_ESS_F_Test(fake_predictions, mid_predictions, actuals, DFFull, DFMid)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "F-stat:  0.544848942979\n",
        "The probability of observing such an extreme result is:  0.800101037327\n"
       ]
      }
     ],
     "prompt_number": 105
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "OK. Well, guys...We"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum_dif = 0\n",
      "for i, j in zip(mid_predictions, mid_predictions):\n",
      "    sum_dif += abs(i-j)\n",
      "print sum_dif\n",
      "\n",
      "sum_dif=0\n",
      "for i, j in zip(full_predictions, red_pred):\n",
      "    sum_dif += abs(i-j)\n",
      "print sum_dif\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.0\n",
        "226.493236233\n"
       ]
      }
     ],
     "prompt_number": 92
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}