{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "SKY Investments\n",
      "===============\n",
      "\n",
      "Sam Fisher, Karl Krehbiel, Yuechen Zhao\n",
      "\n",
      "Overview and Motivation\n",
      "-----------------------\n",
      "The goal of our project is, very simply, to come up with a way of predicting stock market movements.  The motivation for this is that if we can predict stock market movements, then we can make money off of this and... well, frankly, I'm somewhat unconvinced that having money will lead to anything that is of real value... but it's a fun exercise.  The way that we have chosen to attempt to predict these movements is by a combination of google search trend data and previous stock prices.  We are going to try and predict stock price movements based on google search trend data on words related to the company (e.g. \"apple,\" \"iPhone,\" and \"OSX\" for the company Apple)\n",
      "\n",
      "Related Work\n",
      "------------\n",
      "\n",
      "Initial Questions\n",
      "-----------------\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Data\n",
      "----\n",
      "\n",
      "### Raw Data Collection\n",
      "Our data comes exclusively from Google Trends and Yahoo Finance.\n",
      "\n",
      "To extract data from Google Trends, we used an API that Sal Uryasev developed to log into Google and trick Google into allowing us to download their csv data programmatically (they do not provide an API and make it very difficult for developers to get this data). However, the API does not work because Google constantly updates its authentication and other aspects of the download process so that any unofficial APIs get outdated very quickly. We took the API and fixed the issue (specifically with finding the GALX value on the Google log in page) by using pattern.web to extract the authentication information. Then, we updated the API with some calls that help us download data quickly. To see the code we use to download data from Google Trends, see the `gtrends` folder. The function call we use to save any data we download is `gtrends.getGoogleTrendData(<keyword>, <file location>, <date>)`, where `keyword` is the keyword we want to find Google Trend data for, `file location` is where the csv downloaded is to be saved, and `date` specifies the timeframe for which we want the data.\n",
      "\n",
      "To extract data from Yahoo Finance, we use a Python API developed by Corey Goldberg. This API was very well developed and documented, and we did not have to modify it at all. The API is written in `yahoo_stock.py`. We specifically use the function `yahoo_stock.get_historical_prices(<symbol>, <start date>, <end date>)`, which gives us the stock prices available for a certain stock symbol (such as 'AAPL') between the start and end dates.\n",
      "\n",
      "### Cleaning, Associating, and Aggregating the Data\n",
      "We quickly realized that in order to get daily data from Google Trends, we had to request the data month by month. This meant that we were forced to do that for each year and for each month, then aggregate that data together to create our entire dataset for just ONE company. Below is the code we used to download that data (month by month) for Apple."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gtrends import gtrends\n",
      "import yahoo_stock\n",
      "import pandas as pd\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "save_google_data(search_term)\n",
      "\n",
      "Saves the Google Trend Search data for the specified search term for every month\n",
      "from Jan 2004 to Dec 2013. Saves them in data/search_term_date.csv.\n",
      "\n",
      "Arguments\n",
      "=========\n",
      "search_term (string):\n",
      "    The search term for which to download the Google data.\n",
      "\n",
      "Returns\n",
      "=======\n",
      "This function does not return a value.\n",
      "\"\"\"\n",
      "def save_google_data(search_term):\n",
      "    months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
      "    years = [str(y) for y in range(2004, 2014)]\n",
      "    \n",
      "    for year in years:\n",
      "        for month in months:\n",
      "            # skip Dec of 2013 as there's no data!\n",
      "            if year == '2013' and month == '12':\n",
      "                continue\n",
      "            \n",
      "            # calculate date and filename, and save the csv file\n",
      "            date = year + '-' + month\n",
      "            filename = \"data/\" + search_term + \"_\" + date + \".csv\"\n",
      "            gtrends.getGoogleTrendData(search_term, filename, date)\n",
      "\n",
      "# Save data for Apple\n",
      "save_google_data('AAPL', 'apple')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once we downloaded that data from Google, we were ready to aggregate them. However, we also wanted to associate the stock prices with each day, in addition to the Google Trend result. Below is the code we used to generate a dataframe with the Google Search data and Yahoo Stock data combined."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "generate_dataframe(search_term)\n",
      "\n",
      "Generates a dataframe with Google Trend data and price information of a stock linked by date.\n",
      "\n",
      "Arguments\n",
      "=========\n",
      "ticker (string):\n",
      "    The stock symbol (ticker) for the company.\n",
      "\n",
      "search_term (string):\n",
      "    The search term for which we have downloaded Google Data and want to extract.\n",
      "\n",
      "Returns\n",
      "=======\n",
      "Returns the constructed DataFrame.\n",
      "\"\"\"\n",
      "def generate_dataframe(ticker, search_term):\n",
      "    months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
      "    years = [str(y) for y in range(2004, 2014)]\n",
      "\n",
      "    # get the stock info for this ticker from Jan 2004 to Dec 2013\n",
      "    stock_info = yahoo_stock.get_historical_prices(ticker, '20040101', '20131201')\n",
      "    df = pd.DataFrame(stock_info[1:])\n",
      "    df.columns = stock_info[0]\n",
      "    df = df.set_index('Date')\n",
      "    \n",
      "    # we have to combine the data from many different months of Google Trend data\n",
      "    all_time_data = pd.DataFrame()\n",
      "    for year in years:\n",
      "        for month in months:\n",
      "            # skip Dec of 2013 as there's no data!\n",
      "            if year == '2013' and month == '12':\n",
      "                continue\n",
      "            \n",
      "            # create a dataframe out of the month's data\n",
      "            date = year + '-' + month\n",
      "            filename = \"data/\" + search_term + \"_\" + date + \".csv\"\n",
      "            month_data = pd.DataFrame.from_csv(filename)\n",
      "            month_data.columns = ['Trend']\n",
      "            \n",
      "            # aggregate all the month's data into one dataframe\n",
      "            all_time_data = pd.concat([all_time_data, month_data])\n",
      "\n",
      "    # combine stock data with the Google Trend search data\n",
      "    df = df.join(all_time_data, how=\"outer\")\n",
      "\n",
      "    # clean the data and columns\n",
      "    df['Ticker'] = pd.Series([ticker] * len(df), index=df.index)\n",
      "    df = df.reset_index()\n",
      "    new_columns = list(df.columns)\n",
      "    new_columns[0] = 'Date'\n",
      "    df.columns = new_columns\n",
      "    df.Date = df.Date.apply(lambda d: d.strftime('%Y-%m-%d'))\n",
      "    \n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "apple_complete_data = generate_dataframe('AAPL', 'apple')\n",
      "apple_complete_data.head(20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Date</th>\n",
        "      <th>Open</th>\n",
        "      <th>High</th>\n",
        "      <th>Low</th>\n",
        "      <th>Close</th>\n",
        "      <th>Volume</th>\n",
        "      <th>Adj Clos</th>\n",
        "      <th>Trend</th>\n",
        "      <th>Ticker</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0 </th>\n",
        "      <td> 2004-01-01</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>      NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  87</td>\n",
        "      <td> AAPL</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1 </th>\n",
        "      <td> 2004-01-02</td>\n",
        "      <td> 21.55</td>\n",
        "      <td> 21.75</td>\n",
        "      <td> 21.18</td>\n",
        "      <td> 21.28</td>\n",
        "      <td>  5165800</td>\n",
        "      <td> 10.2</td>\n",
        "      <td>  85</td>\n",
        "      <td> AAPL</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2 </th>\n",
        "      <td> 2004-01-03</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>      NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  82</td>\n",
        "      <td> AAPL</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3 </th>\n",
        "      <td> 2004-01-04</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>      NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  80</td>\n",
        "      <td> AAPL</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4 </th>\n",
        "      <td> 2004-01-05</td>\n",
        "      <td> 21.42</td>\n",
        "      <td> 22.39</td>\n",
        "      <td> 21.42</td>\n",
        "      <td> 22.17</td>\n",
        "      <td> 14107800</td>\n",
        "      <td> 10.7</td>\n",
        "      <td>  80</td>\n",
        "      <td> AAPL</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5 </th>\n",
        "      <td> 2004-01-06</td>\n",
        "      <td> 22.25</td>\n",
        "      <td> 22.42</td>\n",
        "      <td> 21.71</td>\n",
        "      <td> 22.09</td>\n",
        "      <td> 18191000</td>\n",
        "      <td> 10.6</td>\n",
        "      <td> 100</td>\n",
        "      <td> AAPL</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6 </th>\n",
        "      <td> 2004-01-07</td>\n",
        "      <td> 22.10</td>\n",
        "      <td> 22.83</td>\n",
        "      <td> 21.93</td>\n",
        "      <td> 22.59</td>\n",
        "      <td> 20959800</td>\n",
        "      <td> 10.9</td>\n",
        "      <td>  93</td>\n",
        "      <td> AAPL</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7 </th>\n",
        "      <td> 2004-01-08</td>\n",
        "      <td> 22.84</td>\n",
        "      <td> 23.73</td>\n",
        "      <td> 22.65</td>\n",
        "      <td> 23.36</td>\n",
        "      <td> 16439400</td>\n",
        "      <td> 11.2</td>\n",
        "      <td>  92</td>\n",
        "      <td> AAPL</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8 </th>\n",
        "      <td> 2004-01-09</td>\n",
        "      <td> 23.23</td>\n",
        "      <td> 24.13</td>\n",
        "      <td> 22.79</td>\n",
        "      <td> 23.00</td>\n",
        "      <td> 15266400</td>\n",
        "      <td> 11.1</td>\n",
        "      <td>  87</td>\n",
        "      <td> AAPL</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9 </th>\n",
        "      <td> 2004-01-10</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>      NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  91</td>\n",
        "      <td> AAPL</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td> 2004-01-11</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>      NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  89</td>\n",
        "      <td> AAPL</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td> 2004-01-12</td>\n",
        "      <td> 23.25</td>\n",
        "      <td> 24.00</td>\n",
        "      <td> 23.10</td>\n",
        "      <td> 23.73</td>\n",
        "      <td> 17412400</td>\n",
        "      <td> 11.4</td>\n",
        "      <td>  80</td>\n",
        "      <td> AAPL</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td> 2004-01-13</td>\n",
        "      <td> 24.70</td>\n",
        "      <td> 24.84</td>\n",
        "      <td> 23.86</td>\n",
        "      <td> 24.12</td>\n",
        "      <td> 24250600</td>\n",
        "      <td> 11.6</td>\n",
        "      <td>  76</td>\n",
        "      <td> AAPL</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13</th>\n",
        "      <td> 2004-01-14</td>\n",
        "      <td> 24.40</td>\n",
        "      <td> 24.54</td>\n",
        "      <td> 23.78</td>\n",
        "      <td> 24.20</td>\n",
        "      <td> 22144400</td>\n",
        "      <td> 11.7</td>\n",
        "      <td>  82</td>\n",
        "      <td> AAPL</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14</th>\n",
        "      <td> 2004-01-15</td>\n",
        "      <td> 22.91</td>\n",
        "      <td> 23.40</td>\n",
        "      <td> 22.50</td>\n",
        "      <td> 22.85</td>\n",
        "      <td> 36364600</td>\n",
        "      <td> 11.0</td>\n",
        "      <td>  84</td>\n",
        "      <td> AAPL</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15</th>\n",
        "      <td> 2004-01-16</td>\n",
        "      <td> 22.89</td>\n",
        "      <td> 23.04</td>\n",
        "      <td> 22.61</td>\n",
        "      <td> 22.72</td>\n",
        "      <td> 13315000</td>\n",
        "      <td> 10.9</td>\n",
        "      <td>  79</td>\n",
        "      <td> AAPL</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>16</th>\n",
        "      <td> 2004-01-17</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>      NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  88</td>\n",
        "      <td> AAPL</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>17</th>\n",
        "      <td> 2004-01-18</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>      NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  79</td>\n",
        "      <td> AAPL</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>18</th>\n",
        "      <td> 2004-01-19</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>      NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  80</td>\n",
        "      <td> AAPL</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>19</th>\n",
        "      <td> 2004-01-20</td>\n",
        "      <td> 22.67</td>\n",
        "      <td> 22.80</td>\n",
        "      <td> 22.25</td>\n",
        "      <td> 22.73</td>\n",
        "      <td> 11283800</td>\n",
        "      <td> 10.9</td>\n",
        "      <td>  81</td>\n",
        "      <td> AAPL</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "          Date   Open   High    Low  Close    Volume Adj Clos  Trend Ticker\n",
        "0   2004-01-01    NaN    NaN    NaN    NaN       NaN      NaN     87   AAPL\n",
        "1   2004-01-02  21.55  21.75  21.18  21.28   5165800     10.2     85   AAPL\n",
        "2   2004-01-03    NaN    NaN    NaN    NaN       NaN      NaN     82   AAPL\n",
        "3   2004-01-04    NaN    NaN    NaN    NaN       NaN      NaN     80   AAPL\n",
        "4   2004-01-05  21.42  22.39  21.42  22.17  14107800     10.7     80   AAPL\n",
        "5   2004-01-06  22.25  22.42  21.71  22.09  18191000     10.6    100   AAPL\n",
        "6   2004-01-07  22.10  22.83  21.93  22.59  20959800     10.9     93   AAPL\n",
        "7   2004-01-08  22.84  23.73  22.65  23.36  16439400     11.2     92   AAPL\n",
        "8   2004-01-09  23.23  24.13  22.79  23.00  15266400     11.1     87   AAPL\n",
        "9   2004-01-10    NaN    NaN    NaN    NaN       NaN      NaN     91   AAPL\n",
        "10  2004-01-11    NaN    NaN    NaN    NaN       NaN      NaN     89   AAPL\n",
        "11  2004-01-12  23.25  24.00  23.10  23.73  17412400     11.4     80   AAPL\n",
        "12  2004-01-13  24.70  24.84  23.86  24.12  24250600     11.6     76   AAPL\n",
        "13  2004-01-14  24.40  24.54  23.78  24.20  22144400     11.7     82   AAPL\n",
        "14  2004-01-15  22.91  23.40  22.50  22.85  36364600     11.0     84   AAPL\n",
        "15  2004-01-16  22.89  23.04  22.61  22.72  13315000     10.9     79   AAPL\n",
        "16  2004-01-17    NaN    NaN    NaN    NaN       NaN      NaN     88   AAPL\n",
        "17  2004-01-18    NaN    NaN    NaN    NaN       NaN      NaN     79   AAPL\n",
        "18  2004-01-19    NaN    NaN    NaN    NaN       NaN      NaN     80   AAPL\n",
        "19  2004-01-20  22.67  22.80  22.25  22.73  11283800     10.9     81   AAPL"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Building, Testing, and Visualizing\n",
      "----------------------------------\n",
      "\n",
      "There were some limitations of the Google Trend Data that impacted decisions about our predictive model. The main limitation was that all trends were given as percentages of the maximum search volume in a single day that occurred during that month. This made it challenging to compare Google Trends meaningfully between different months. Therefore we decided that at least for now we would make all predictions about Monday opening prices as a function of last week\u2019s Google trends and stock prices. [UPDATE THIS PART]\n",
      "\n",
      "We also figured that the model should not care about the actual stock prices as much as changes in prices, so we decided that we would normalize stock prices, as follows.\n",
      "\n",
      "For predictive attributes for our model, we used:\n",
      "\n",
      "1. Each day\u2019s search volume of keyword as a percentage of the max volume for any given day in the week\n",
      "2. Each day's stock price as percentage of some other price in the week (which we varied in different iterations of our model). \n",
      "\n",
      "For output, we used:\n",
      "\n",
      "1. Next Monday\u2019s stock price as a percentage of the same normalizing price used in the input.\n",
      "\n",
      "We split the data into training and testing sets and built a multiple linear regression and then tested it on the testing set. Then we visualized our results by plotting the predicted percentage (of normalizing price) on the x-axis and the actual percentage (of normalizing price) on the y-axis. This step resulted in some very confusing outcomes (though we eventually made sense of them).\n",
      "\n",
      "First, we used the maximum stock price from the week as the normalizing price. In this trial, we observed a plot with points hovering rather closely to the diagonal line y=x, from which we concluded that our model was good! (Oh, were we ever wrong.)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/images/fig1.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(Note that the percent correct measure was a measure of how many of our predictions moved in the correct direction with respect to the maximum opening price of the week)\n",
      "\n",
      "Then we ran a few more tests. When we omitted the Google trend data from the regression, we ended up with nearly the same outcomes, as shown below."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/images/fig2.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When we omitted the stock data we ended up with essentially random outcomes."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/images/fig3.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From this we concluded that, at least for purposes of this current regression, Google trend data was unhelpful. We confirmed this by printing the coefficients of the regression and observing very small values for the variables corresponding to the Google data.\n",
      "\n",
      "`-0.16286722,  0.14120525, -0.20483681,  0.29103132,  0.76371615, -0.00498012, -0.02829319, -0.05257678,  0.01665766,  0.01858135, -0.08663877,  0.08635663`\n",
      "\n",
      "(the last 7 figures correspond to the coefficients for the Google Trend attributes)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then we tried again using Friday\u2019s stock price as the normalizing price instead of the maximum price from the week. This time, we observed points that were drastically more scattered and that did not seem to hover at all around the diagonal line. We were confused. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/images/fig4.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then we tried normalizing with respect to Thursday, and the plot suggested better results."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/images/fig5.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Wednesday\u2014even better. Tuesday\u2014better yet. And Monday\u2014best of all."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/images/fig6.png\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Why would our model do better or worse depending on the normalizing constant?\n",
      "\n",
      "We tried again, this time giving outcomes that were actual stock prices for next Monday (by multiplying the percentage outcomes\u2014predicted and true\u2014by the normalizing prices for each case) rather than just the percentages themselves. The resulting graph showed prices spanning from around \\$20 to \\$700, so the difference between predictions and actual prices looked miniscule (the visualization was unhelpful, as can be seen below)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/images/fig7.png\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So we summed the absolute differences between predicted prices and true prices over the test set. Then we tried again to vary the normalizing prices. Now we observed that the error did not vary on the basis of which price we used to normalize."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Squared Error: 1403.21091956 (Friday's price as normalizing price)\n",
      "\n",
      "Squared Error: 1466.07107308 (Monday's price as normalizing price)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our question was clear: Why did predictions of percentage changes in price perform much better with respect to older price while predictions of actual prices did not?\n",
      "\n",
      "After thinking it over for a while, we figured it out.\n",
      "When predicting percentage changes with respect to last Monday, the price change between Friday and next Monday was only a small part of the story in predicting percentage change from last Monday to next Monday, since usually the majority of change occurred between last Monday and Friday. Since any error in our model only misrepresented the change between Friday and next Monday, the percentage prediction with respect to last Monday was mostly correct, since it knew how the stock ended up after four days and only needed to predict one more. In other words, since the model knew where the stock was on Friday and could estimate that it would be in the same area on next Monday, it could give a good estimate of how next Monday\u2019s stock compared to last Monday\u2019s stock. However, when predicting percentage changes with respect to Friday, the price change between Friday and next Monday represented immediately impacted the entire prediction, so any error in our model had enormous impact on the prediction. If our model was wrong, the prediction was completely wrong. In other words, the model didn\u2019t know any values that could give \u2018hints\u2019 that were observed between Friday and next Monday, and so was essentially stabbing in the dark.\n",
      "\n",
      "Moreover, thinking back, it was probably a poor decision to devote our model to predicting Mondays based on previous weeks because prices are not reported over the weekend, and so the Monday open price is probably less predictable than the opening prices of other days. The reason we originally decided to predict Monday prices was that we needed to slice our data somehow casually figured it made sense to predict the first day of each week without considering the fact that the market is closed over the weekend.\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}